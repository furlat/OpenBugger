{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import libcst as cst\n",
    "import re\n",
    "import random\n",
    "from openbugger.bugger import Bugger, bugger_example\n",
    "from time import perf_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all bugs from openbugger\n",
    "from openbugger.bugs.controlflow import ForgettingToUpdateVariableTransformer, InfiniteWhileTransformer, gen_OffByKIndexTransformer, IncorrectExceptionHandlerTransformer,MissingArgumentTransformer,ReturningEarlyTransformer\n",
    "from openbugger.bugs.data import IncorrectVariableInitializationTransformer, VariableNameTypoTransformer, MutableDefaultArgumentTransformer, UseBeforeDefinitionTransformer\n",
    "from openbugger.bugs.logical import gen_ComparisonTargetTransfomer, ComparisonSwapTransformer\n",
    "from openbugger.bugs.type import IncorrectTypeTransformer, NonExistingMethodTransformer, SwapForTransformer\n",
    "from openbugger.bugs.numpy import NumpyArrayCreationTransformer, NumpyMethodMisuseTransformer, NumpyReshapeMisuseTransformer, NumpyArangeMisuseTransformer, NumpyAxisMisuseTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from babydragon.models.generators.chatgpt import chatgpt_response\n",
    "from babydragon.utils.chatml import get_str_from_response, mark_question\n",
    "import openai\n",
    "openai.api_key = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_debugger_prompt(problem_definition, correct_code, bug_analysis, user_questions):\n",
    "    bugged_code = bug_analysis['bugged_code']\n",
    "    # Prepare the bug fix information based on the bug_analysis\n",
    "    bug_analysis = bug_analysis['bugs']\n",
    "    bugs = bug_analysis['bugs']\n",
    "    fixes = bug_analysis['cleans']\n",
    "    bug_positions = bug_analysis['positions']\n",
    "    bug_fix_info = list(zip(bug_positions, bugs, fixes))\n",
    "    bug_fix_content = '\\n'.join([f\"At line {pos[0]}, we have a bug '{bug}' which can be fixed by replacing it with '{fix}'.\" \n",
    "                                 for pos, bug, fix in bug_fix_info])\n",
    "\n",
    "    # Create a comprehensive prompt for GPT-4\n",
    "    prompt = (f\"As a skilled software developer, you are working on the following algorithmic task:\\n\\n\"\n",
    "              f\"{problem_definition}\\n\\n\"\n",
    "              f\"The correct implementation of this task should look something like this:\\n\\n\"\n",
    "              f\"{correct_code}\\n\\n\"\n",
    "              f\"However, you are currently dealing with a version of the code that contains some bugs:\\n\\n\"\n",
    "              f\"{bugged_code}\\n\\n\"\n",
    "              f\"The auto-bugger tool has detected these bugs and suggests how to fix them:\\n\\n\"\n",
    "              f\"{bug_fix_content}\\n\\n\"\n",
    "              f\"A programmer has the following questions about the code:\\n\\n\"\n",
    "              f\"{user_questions}\\n\\n\"\n",
    "              f\"Can you help answer these questions, identify the bugs, and suggest how to correct them? Please guide them through the debugging process, explaining each bug fix in detail.\")\n",
    "\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_user_prompt(level, problem_definition, bugged_code):\n",
    "    common_intro = (f\"You are a programmer who has been tasked with solving a specific problem. \"\n",
    "                    f\"However, as you are trying to understand and debug a given piece of code, you encounter some issues. \"\n",
    "                    f\"In your response, start by situating yourself in your role as a programmer at the {level} level and express the difficulties you're facing with the code. Remember to ask max three, 3 questions\\n\\n\"\n",
    "                    f\"Here's the problem you're working on:\\n\\n\"\n",
    "                    f\"{problem_definition}\\n\\n\"\n",
    "                    f\"And here's the code you're trying to understand and debug:\\n\\n\"\n",
    "                    f\"Bugged code:\\n{bugged_code}\\n\\n\")\n",
    "\n",
    "    level_prompts = {\n",
    "        \"beginner\": (f\"As a beginner, you're just getting started with learning about algorithms. \"\n",
    "                     f\"Ask a series of questions to help you understand what this code is trying to do, where it might be going wrong, \"\n",
    "                     f\"and what concepts or techniques you might need to learn to understand it better. There are no silly questions at this stage!\"),\n",
    "\n",
    "        \"intermediate\": (f\"As an intermediate programmer, you already have a good understanding of coding principles. \"\n",
    "                         f\"Formulate a series of questions that delve into the specific implementation and possible logical errors in the code \"\n",
    "                         f\"that would help you identify the problems. Your questions should reflect your ability to apply problem-solving strategies.\"),\n",
    "\n",
    "        \"expert\": (f\"As an expert programmer, you are accustomed to dealing with complex algorithmic challenges. \"\n",
    "                   f\"Ask expert-level questions about the subtle nuances in the code and the underlying algorithm, \"\n",
    "                   f\"and propose possible debugging strategies based on your analysis of the code. Your questions should reflect a deep understanding of the algorithm and potential edge cases.\")\n",
    "    }\n",
    "\n",
    "    return common_intro + level_prompts.get(level, \"Invalid level. Please choose between 'beginner', 'intermediate' or 'expert'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from threading import Lock\n",
    "import functools\n",
    "from typing import List, Dict\n",
    "from babydragon.utils.multithreading import RateLimiter, RateLimitedThreadPoolExecutor\n",
    "import os\n",
    "\n",
    "\n",
    "class ParallelLeetBugCreator:\n",
    "    def __init__(self, max_workers: int = None, calls_per_minute: int = 20, verbose: bool = False, backup: bool = False):\n",
    "        self.executor = RateLimitedThreadPoolExecutor(max_workers, calls_per_minute=calls_per_minute, verbose=verbose)\n",
    "        self.backup = backup\n",
    "        self.results = []\n",
    "        self.failed_sub_tasks = []\n",
    "        \n",
    "    def load_dataset(self, dataset_name: str = f\"mhhmm/leetcode-solutions-python\", split: str = \"train\"):\n",
    "        docs = load_dataset(dataset_name, split=split)\n",
    "        return docs\n",
    "    \n",
    "    def save_to_json(self, data, filename):\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(data, f)\n",
    "\n",
    "    def clean_code_and_extract(self, docs):\n",
    "        code_list = []\n",
    "        problem_list = []\n",
    "        data_list = []\n",
    "        cst_list = []\n",
    "        bad_code = []\n",
    "        explanation_list = []\n",
    "        for code_with_data,code_with_problem,code,explanation in zip(docs[\"code_with_data\"],docs[\"code_with_problem\"],docs[\"code_only\"],docs[\"explanation_only\"]):\n",
    "            # Remove ```python and ``` at the end \n",
    "            good_code = False\n",
    "            clean_code = re.sub(r'```python', '', code)\n",
    "            clean_code = re.sub(r'```.*', '', clean_code).strip()\n",
    "\n",
    "            split_data = code_with_data.split('```python')\n",
    "            split_problem = code_with_problem.split('```python')\n",
    "            if len(split_data) > 1 and len(split_problem) > 1:\n",
    "                data = split_data[0]\n",
    "                problem = split_problem[0]\n",
    "                \n",
    "                try:\n",
    "                    cst_tree = cst.parse_module(clean_code)\n",
    "                    good_code = True\n",
    "                    code_list.append(clean_code)\n",
    "                    cst_list.append(cst_tree)\n",
    "                    data_list.append(data)\n",
    "                    problem_list.append(problem)\n",
    "                    explanation_list.append(explanation)\n",
    "                except:\n",
    "                    bad_code.append(clean_code)\n",
    "        return code_list, problem_list, data_list, cst_list, bad_code, explanation_list\n",
    "\n",
    "    def load_bugs(self, url: str):\n",
    "        with open(url, 'r') as file:\n",
    "            all_bugs = json.load(file)\n",
    "        return all_bugs\n",
    "\n",
    "    def create_qa_pair(self,correct_code,problem_definition,bug_analysis):\n",
    "        bugged_code = bug_analysis['bugged_code']\n",
    "        user_prompt = generate_user_prompt(level=\"beginner\", problem_definition=problem_definition, bugged_code=bugged_code)\n",
    "        response_user, status = chatgpt_response(prompt = [mark_question(user_prompt)], model = \"gpt-3.5-turbo\")\n",
    "        if status:\n",
    "            user_questions = get_str_from_response(response_user)\n",
    "            debugger_prompt = generate_debugger_prompt( problem_definition = problem_definition, correct_code = correct_code , bug_analysis = bug_analysis, user_questions = user_questions)\n",
    "            response_debugger, status = chatgpt_response(prompt = [mark_question(debugger_prompt)], model = \"gpt-3.5-turbo\")\n",
    "            if status:\n",
    "                return user_questions, get_str_from_response(response_debugger)\n",
    "            else:\n",
    "                raise Exception(\"Error in chatgpt_response during debugger generation\")\n",
    "        else:\n",
    "            raise Exception(\"Error in chatgpt_response during question generation\")\n",
    "\n",
    "    def _execute_sub_task(self, script: str, problem_definition: str, bug: Dict):\n",
    "        # print(\"the bug keys are\", bug[\"bugs\"].keys())   \n",
    "        try:\n",
    "            question, debugger = self.create_qa_pair(correct_code=script, problem_definition=problem_definition, bug_analysis=bug)\n",
    "            return {\"question\": question, \"answer\": debugger}\n",
    "        except Exception as e:\n",
    "            return f\"Error in bug creation: {e}\"\n",
    "\n",
    "    def execute_task(self, code_list: List[str], problem_list: List[str], all_bugs: Dict):\n",
    "        output_dir = \"./outs\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        with self.executor as executor:\n",
    "            futures = []\n",
    "            print(f\"Executing task using {self.executor._max_workers} workers.\")\n",
    "\n",
    "            for i, transformers in all_bugs.items():\n",
    "                i = int(i)\n",
    "                for name, bug in transformers.items():\n",
    "                    # Create path to expected output JSON\n",
    "                    output_file = os.path.join(output_dir, f\"{i}_{name}.json\")\n",
    "                \n",
    "                    # Check if output JSON already exists, if so, skip execution\n",
    "                    if os.path.exists(output_file) and self.backup:\n",
    "                        #read the json and check it it staarts with Bug if starts with bug do not skip else skip\n",
    "                        with open(output_file, 'r') as f:\n",
    "                            data = json.load(f)\n",
    "                        if isinstance(data,str) and data.startswith(\"Error\"):\n",
    "                            print(f\"JSON file for bug {name} at index {i} already exists but is bugged, exevuting again.\")\n",
    "                        else:\n",
    "                            # print(f\"JSON file for bug {name} at index {i} already exists, skipping execution.\")\n",
    "                            continue\n",
    "                    script = code_list[i]\n",
    "                    problem_definition = problem_list[i]\n",
    "                    future = executor.submit(self._execute_sub_task, script, problem_definition, bug)\n",
    "                    futures.append((i, name, future))\n",
    "\n",
    "            for i, name, future in futures:\n",
    "                try:\n",
    "                    execution_start_time = time.time()\n",
    "                    bug_result = future.result()\n",
    "                    execution_end_time = time.time()\n",
    "                    print(\n",
    "                        f\"Bug {name} at index {i} executed in {execution_end_time - execution_start_time:.2f} seconds.\"\n",
    "                    )\n",
    "\n",
    "                    save_start_time = time.time()\n",
    "                    self.results.append(bug_result)\n",
    "                    if self.backup:\n",
    "                        # Save each QA pair as a separate JSON file\n",
    "                        output_file = os.path.join(output_dir, f\"{i}_{name}.json\")\n",
    "                        self.save_to_json(bug_result, output_file)\n",
    "                    save_end_time = time.time()\n",
    "                    print(\n",
    "                        f\"Bug {name} at index {i} results saved in {save_end_time - save_start_time:.2f} seconds.\"\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in bug {name} at index {i}: {e}\")\n",
    "                    default_result = {name: f\"Error in bug {name} at index {i}: {e}\"}\n",
    "                    self.results.append(default_result)\n",
    "                    if self.backup:\n",
    "                        output_file = os.path.join(output_dir, f\"{i}_{name}_error.json\")\n",
    "                        self.save_to_json(default_result, output_file)\n",
    "                    self.failed_sub_tasks.append((i, name, str(e)))\n",
    "\n",
    "                except KeyboardInterrupt:\n",
    "                    print(\"Keyboard interrupt detected, stopping task execution.\")\n",
    "                    executor.shutdown(wait=False)\n",
    "                    break\n",
    "\n",
    "        print(\"Task execution completed.\")\n",
    "    def work(self):\n",
    "        docs = self.load_dataset()\n",
    "        code_list, problem_list, data_list, cst_list, bad_code, explanation_list = self.clean_code_and_extract(docs)\n",
    "        all_bugs = self.load_bugs(\"bugger_all_bugs_leetcode.json\")\n",
    "        self.execute_task(code_list, problem_list, all_bugs)\n",
    "        return self.results, self.failed_sub_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/Users/iridella/.cache/huggingface/datasets/mhhmm___json/mhhmm--leetcode-solutions-python-c6d2758e3a6cc905/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing task using 12 workers.\n",
      "JSON file for bug OffByKIndexTransformer at index 1441 already exists but is bugged, exevuting again.\n",
      "RateLimiter: This is the first call, no wait required.\n",
      "Trying to call OpenAI API...\n",
      "JSON file for bug OffByKIndexTransformer at index 1545 already exists but is bugged, exevuting again.\n",
      "RateLimiter: Waiting for 1.43 seconds before next call.\n",
      "Trying to call OpenAI API...\n",
      "Trying to call OpenAI API...\n",
      "Bug OffByKIndexTransformer at index 1441 executed in 10.19 seconds.\n",
      "Bug OffByKIndexTransformer at index 1441 results saved in 0.00 seconds.\n",
      "Trying to call OpenAI API...\n",
      "Bug OffByKIndexTransformer at index 1545 executed in 0.78 seconds.\n",
      "Bug OffByKIndexTransformer at index 1545 results saved in 0.00 seconds.\n",
      "Task execution completed.\n"
     ]
    }
   ],
   "source": [
    "bug_creator = ParallelLeetBugCreator(max_workers=12, calls_per_minute=40, verbose=True, backup=True)\n",
    "results, failed_sub_tasks = bug_creator.work()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
